{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** - это абстрактный класс определяющий необходимые для нейронной сети методы. В этой части не нужно ничего писать, просто читайте комментарии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \"\"\"\n",
    "    В целом, Module это черный ящик, который принимает что-то на вход и\n",
    "    выдает какой-то результат. Это результат применения метода \"forward\":\n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    Также этот черный ящик должен уметь по значеню dL/dOutput подсчитывать\n",
    "    производную функции ошибки по параметрам модели (dL/dW) и по входным данным\n",
    "    (dL/dInput)\n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Вычислает по данному input соответствующий output, сохраняя его в одноименном\n",
    "        поле себя.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Пример для тождественной функции: \n",
    "        \n",
    "        # self.output = input\n",
    "        # return self.output\n",
    "        \n",
    "        assert False, 'Implementation error'\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Делает шаг обратного распространения ошибки.\n",
    "        \n",
    "        Включает в себя:\n",
    "        1) подсчет градиента функции ошибки по входу (для выполнения\n",
    "           шага обратного распространения ошибки предыдущего слоя)\n",
    "        2) подсчет градиента функции ошибки по параметрам (для \n",
    "           выполнения шага градиентного спуска)\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Подсчитывает градиент функции ошибки по входу, сохраняет результат\n",
    "        в поле gradInput. Размерность полученного градиента всегда строго совпадает\n",
    "        с размерностью входа.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Для тождественной функции:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        assert False, 'Implementation error'\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Подсчитывает градиент функции потерь по параметрам\n",
    "        \n",
    "        Не нужно реализовывать если у модуля нет параметров (ReLU)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Обнуляет градиент\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список подгоняемых (trainable) параметров.\n",
    "        \n",
    "        Если таковых нет, то возвращает пустой список.\n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список градиентов функции потерь по подгоняемым параметрам.\n",
    "        \n",
    "        Если таковых нет, то возвращает пустой список.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Переводит модуль в режим обучение. Влияет только на дропаут и batchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Отключает режим обучения. Влияет только на дропаут и batchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Выводит имя модуля.\n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         Это контейнер, который для данного входа последовательно применяет\n",
    "         все переданные в него модули (выход предущего модуля - вход следующего)\n",
    "         \n",
    "         Итоговое значение будет являться выходом данного контейнера.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Добавляет модуль в контейнер.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Вычислает по данному input соответствующий output, сохраняя его в одноименном\n",
    "        поле себя.\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs = []\n",
    "        last_output = input\n",
    "        \n",
    "        for i, mod in enumerate(self.modules):\n",
    "            self.inputs.append(last_output)\n",
    "            last_output = mod.forward(self.inputs[-1])\n",
    "            \n",
    "        self.output = last_output\n",
    "                \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Реализует backward pass:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "        \n",
    "        \"\"\"\n",
    "        yinp = self.inputs[::-1]\n",
    "        t = gradOutput\n",
    "        \n",
    "        for i, mod in enumerate(self.modules[::-1]):\n",
    "            t = mod.backward(yinp[i], t)\n",
    "                \n",
    "        self.gradInput = t\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список списков параметров каждого модуля.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Возвращает список списков градиентов функции потерь по параметрам каждого модуля.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Модуль выполняющий линейное преобразование над входом. Также известен\n",
    "    как полносвязный слой.\n",
    "    \n",
    "    Модуль должен принимать на вход матрицу размерности (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        \n",
    "    def dumb_forward(self, input):\n",
    "        '''\n",
    "        Считает выход линейного слоя по данному входу, сохраняет результат в self.output\n",
    "        и вовзращает self.output.\n",
    "        output[object_idx, out_neuron_idx] = \n",
    "        \\sum_{in_neuron_idx} input[object_idx,n] * w_[out_neuron_idx, in_neuron_idx]\n",
    "        '''\n",
    "        n_obj = len(input)\n",
    "        n_out, n_in = self.W.shape\n",
    "        self.output = np.zeros((n_obj, n_out))\n",
    "        for i_obj in range(n_obj):\n",
    "            for i_out in range(n_out):\n",
    "                self.output[i_obj, i_out] = 0\n",
    "                for i_in in range(n_in):\n",
    "                    self.output[i_obj, i_out] += self.W[i_out][i_in] *\\\n",
    "                                            input[i_obj][i_in]\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = np.dot(self.W, input.T).T\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        '''\n",
    "        Считает градиент функции потерь по входу, \n",
    "        сохраняет его в поле self.gradInput. Размерность\n",
    "        этого градиента равна размерности input.\n",
    "        \n",
    "        Получает на вход input, использованный при вызове\n",
    "        метода forward, и gradOutput, массив состоящий из\n",
    "        производных функции потерь по каждому из выходных\n",
    "        значений (размер совпадает с размером self.output)\n",
    "        '''\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        '''\n",
    "        Считает градиент функции потерь по параметрам (весам)\n",
    "        слоя, сохраняет его в поле self.gradParameters. Размерность\n",
    "        этого градиента равна размерности self.W.\n",
    "        \n",
    "        Получает на вход input, использованный при вызове\n",
    "        метода forward, и gradOutput, массив состоящий из\n",
    "        производных функции потерь по каждому из выходных\n",
    "        значений (размер совпадает с размером self.output)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        assert False, \"Implementation error\"\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        self.output = np.exp(self.output)\n",
    "        self.output /= self.output.sum(axis=1).reshape(-1,1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        gradOutput = gradOutput.copy()\n",
    "        gradOutput = gradOutput * self.output\n",
    "        self.gradInput = -self.output * gradOutput.sum(axis=1).reshape(-1,1) + gradOutput\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU **Rectified Linear Unit** реализован за вас: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = 1/(1 + np.exp(-input))\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = self.output * (1 - self.output) * gradOutput\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Считает ошибку, сохраняет ее и возвращает.\n",
    "        \"\"\"\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Считает градиент ошибки по input, сохраняет его в gradInput\n",
    "            и возвращает.\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, target)\n",
    "        return self.gradInput\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        assert False, \"Implementation error\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Выводит название функции потерь в человекочитаемом виде.\n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь **MSECriterion**, среднеквадратичная ошибка, реализована за вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def forward(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
